{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of features example with scikit-learn\n",
    "\n",
    "This file serves as simple example of how to use some of scikit-learn capabilities for feature selection and how to chain various algorithms for classification/regression.\n",
    "\n",
    "First a group of decision trees is used in order to determine how it splits the trees according to an entropy criterion and establishing some form of importance in the features.\n",
    "\n",
    "The second example first uses PCA to transform the representation of the features and then passes the new instances to a regressor (logistic regression/decision tree). Then through grid search it is estimated how many components should the model take.\n",
    "\n",
    "[Decision tree criterions for relative importance](#A1)\n",
    "\n",
    "[PCA and regressor pipeline](#A2)\n",
    "\n",
    "[Component selection](#A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.html.widgets import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "rcParams.update({'font.size': 15})\n",
    "#plt.style.use('ggplot')\n",
    "#plt.style.use('seaborn-dark-palette')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "float_formatter = lambda x: \"%.2f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "import io\n",
    "from IPython.nbformat import current\n",
    "\n",
    "def execute_notebook(nbfile):\n",
    "    \n",
    "    with io.open(nbfile) as f:\n",
    "        nb = current.read(f, 'json')\n",
    "    \n",
    "    ip = get_ipython()\n",
    "    \n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free']\n",
      "250\n",
      "['free', 'train']\n",
      "['free', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "execute_notebook(\"Preprocessing.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = 4\n",
    "catLab = ['1D', 'I1D', '2D', 'R']\n",
    "\n",
    "usersF = np.shape(np.unique(csvIntF[:,0]))[0]#5\n",
    "# For free exploration with Training\n",
    "usersFT = np.shape(np.unique(csvIntFT[:,0]))[0]\n",
    "usersFTI = np.shape(np.unique(informed[:,0]))[0]\n",
    "usersFTU = usersFT-usersFTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load behavioral trajectories (csvIntFT)\n",
    "# Get only those in training phase\n",
    "csvIntFTT = csvIntFT[csvIntFT[:,2]==1]\n",
    "# Get a copy for splitting by condition (below)\n",
    "csvIntFTTCond = csvIntFTT.copy()\n",
    "\n",
    "# Remove phase column (2) (and for now also the condition column (1))\n",
    "csvIntFTT = np.delete(csvIntFTT, (1,2), axis=1)\n",
    "\n",
    "# Split by condition\n",
    "# Get rid of phase column\n",
    "csvIntFTTCond = np.delete(csvIntFTTCond, 2, axis = 1)\n",
    "# Split by informed/uninformed\n",
    "informedFTT = csvIntFTTCond[csvIntFTTCond[:,1]==0]\n",
    "uninformedFTT = csvIntFTTCond[csvIntFTTCond[:,1]==1]\n",
    "informedFTT = np.delete(informedFTT, 1, axis=1)\n",
    "uninformedFTT = np.delete(uninformedFTT, 1, axis=1)\n",
    "# Split by user\n",
    "splitInfFTT = [informedFTT[informedFTT[:,0]==i] for i in np.unique(informedFTT[:,0])]\n",
    "splitUniFTT = [uninformedFTT[uninformedFTT[:,0]==i] for i in np.unique(uninformedFTT[:,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-reported answers preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Exploration - general metrics loaded\n",
      "Free Exploration with Training - general metrics loaded\n",
      "Strategic Learning - general metrics loaded\n"
     ]
    }
   ],
   "source": [
    "execute_notebook(\"Preprocessing-Stack Data.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user(0), cond(1) cat-task complexity(2), # task selec(3), % sele(4), # correct on task(5), % correct(6), \n",
    "#answers(7:12)\n",
    "\n",
    "# Split by users that received information about the existence of a random task and those that didn't\n",
    "informedFT = freeT[freeT[:,1]==0]\n",
    "uninformedFT = freeT[freeT[:,1]==1]\n",
    "# For training only\n",
    "informedFTT = freeTTr[freeTTr[:,1]==0]\n",
    "uninformedFTT = freeTTr[freeTTr[:,1]==1]\n",
    "\n",
    "#print(spilot[-1,:])\n",
    "# Remove column \n",
    "freeT = np.delete(freeT, 1, axis=1)\n",
    "informedFT = np.delete(informedFT, 1, axis=1)\n",
    "uninformedFT = np.delete(uninformedFT, 1, axis=1)\n",
    "informedFTT = np.delete(informedFTT, 1, axis=1)\n",
    "uninformedFTT = np.delete(uninformedFTT, 1, axis=1)\n",
    "free = np.delete(free, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rate of empirical success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bb4891389354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#user(0), cat-task complexity(1), # task selec(2), % sele(3), # correct on task(4), % correct(5),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#answers(6:12), relative performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0minformedFTT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minformedFTT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelRateFTTI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/lib/shape_base.pyc\u001b[0m in \u001b[0;36mcolumn_stack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Rate of success\n",
    "# To use as alternative to the probabilities\n",
    "TRIALS_TRAINING = 15\n",
    "def rateSucc(arr, userArr):\n",
    "    # Store performance per user per task\n",
    "    perfUser = []\n",
    "    # Go through user\n",
    "    for u in range(userArr):\n",
    "        tmpU = arr[u]\n",
    "        perfTask = []\n",
    "        # Go through each task\n",
    "        for t in range(tasks):\n",
    "            # Split by task\n",
    "            tmpT = tmpU[tmpU[:,1] == t]\n",
    "            perfTask.append(np.sum(tmpT[:,2]==1)/TRIALS_TRAINING)\n",
    "        perfUser.append(perfTask)\n",
    "            \n",
    "    return np.asarray(perfUser)\n",
    "\n",
    "def relRateSucc(arr):\n",
    "    return arr/np.mean(arr, axis=1).reshape(np.shape(arr)[0],1)\n",
    "\n",
    "# Obtain predicted accuracy (e.g. \"I predict based on my observations that my p(correct)=? if you ask me \n",
    "# to classify instances of this exercise without receiving any feedback\")\n",
    "rateFTTI = rateSucc(splitInfFTT, usersFTI)\n",
    "rateFTTU = rateSucc(splitUniFTT, usersFTU)\n",
    "\n",
    "relRateFTTI = relRateSucc(rateFTTI)\n",
    "relRateFTTU = relRateSucc(rateFTTU)\n",
    "\n",
    "#user(0), cat-task complexity(1), # task selec(2), % sele(3), # correct on task(4), % correct(5), \n",
    "#answers(6:12), relative performance\n",
    "informedFTT = np.column_stack((informedFTT, relRateFTTI.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability success trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets the p(correct) at each trial per task and per user\n",
    "def extractProbsTrajec(arr, userArr):\n",
    "    # Store p(correct) per user\n",
    "    probUser = []\n",
    "    # Go through user\n",
    "    for u in range(userArr):\n",
    "        tmpU = arr[u]\n",
    "        # Init prior\n",
    "        alpha,bet = 2,2\n",
    "        # Store p(correct) = predicted accuracy evolution per task for user u\n",
    "        probTask = []\n",
    "        # Go through each task\n",
    "        for t in range(tasks):\n",
    "            # Split by task\n",
    "            tmpT = tmpU[tmpU[:,1] == t]\n",
    "            # Go through its trial history\n",
    "            succ = 0\n",
    "            # store p(correct) in this task\n",
    "            probs = []\n",
    "            probs.append(np.mean(beta(alpha+succ, bet+(0+1)-succ).rvs(size=500)))\n",
    "            #print(np.mean(beta(alpha+succ, bet+(0+1)-succ).rvs(size=500)), alpha+succ, bet+0-succ)\n",
    "            for trial in range(np.shape(tmpT)[0]):\n",
    "                # Check if the answer was correct on this trial\n",
    "                if tmpT[trial,2] == 1:\n",
    "                    succ+=1\n",
    "                #Construct beta distribution for posterior Beta(α=prior α+Succ, β=prior β+Trials−Succ)\n",
    "                dist = beta(alpha+succ, bet+(trial+1)-succ)\n",
    "                #Draw sample from beta distribution\n",
    "                #print(np.mean(dist.rvs(size=500)), alpha+succ, bet+(trial+1)-succ)\n",
    "                probs.append(np.mean(dist.rvs(size=500)))\n",
    "            probTask.append(probs)\n",
    "        probUser.append(probTask)\n",
    "    return np.asarray(probUser)\n",
    "\n",
    "# Obtain predicted accuracy (e.g. \"I predict based on my observations that my p(correct)=? if you ask me \n",
    "# to classify instances of this exercise without receiving any feedback\")\n",
    "probsFTTI = extractProbsTrajec(splitInfFTT, usersFTI)\n",
    "probsFTTU = extractProbsTrajec(splitUniFTT, usersFTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively use empirical rate of success trajectory (probably noisier during the first trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractSuccTrajec(arr, userArr):\n",
    "    # Store rate of success per user\n",
    "    succUser = []\n",
    "    # Go through user\n",
    "    for u in range(userArr):\n",
    "        tmpU = arr[u]\n",
    "        # Store evolution rate of success per task\n",
    "        succTask = []\n",
    "        # Go through each task\n",
    "        for t in range(tasks):\n",
    "            # Split by task\n",
    "            tmpT = tmpU[tmpU[:,1] == t]\n",
    "            # Go through its trial history\n",
    "            succ = 0\n",
    "            # store rates of success in this task\n",
    "            rates = []\n",
    "            # Added initial 0 so when transformed to error it starts from 1. However this value doesn't affect rate \n",
    "            # of success\n",
    "            rates.append(succ)\n",
    "            for trial in range(np.shape(tmpT)[0]):\n",
    "                # Check if the answer was correct on this trial\n",
    "                if tmpT[trial,2] == 1:\n",
    "                    succ+=1\n",
    "                rates.append(succ / (trial+1) )\n",
    "            succTask.append(rates)\n",
    "        succUser.append(succTask)\n",
    "    return np.asarray(succUser)\n",
    "\n",
    "succFTTI = extractSuccTrajec(splitInfFTT, usersFTI)\n",
    "succFTTU = extractSuccTrajec(splitUniFTT, usersFTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-47855bb2bf36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#user(0), cat-task complexity(1), # task selec(2), % sele(3), # correct on task(4), % correct(5),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#answers(6:12), relative performance, lp, relative lp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0minformedFTT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minformedFTT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlpFTTI_N\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlpRelFTTI_N\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/lib/shape_base.pyc\u001b[0m in \u001b[0;36mcolumn_stack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "def getLPNormal(rates, userArr):\n",
    "    lpsUser = []\n",
    "    # Go through every user and task\n",
    "    for u in range(userArr):\n",
    "        #tmpU = diff[u]\n",
    "        tmpU = rates[u]\n",
    "        lps = []\n",
    "        for t in range(tasks):\n",
    "            # Convert prob to errors\n",
    "            errT = 1-tmpU[t]\n",
    "            # fit polinomial to error differences\n",
    "            slope, intercept = np.polyfit(np.arange(len(errT)), errT, 1)\n",
    "            x = np.linspace(0, len(errT)-1, 100)\n",
    "            # LP = - [Fitted Error(present) - Fitted Error(past)]\n",
    "            lp = -((slope*x[-1]+intercept)-(slope*x[0]+intercept))\n",
    "            lps.append(lp)\n",
    "            #print(slope*x[0]+intercept, slope*x[-1]+intercept, diffT, lp)\n",
    "            #print(probs[u], errT, diffT, slope*x[0]+intercept, slope*x[-1]+intercept, lp)\n",
    "        lpsUser.append(lps)\n",
    "        \n",
    "        #print(lpsUser)\n",
    "    return np.asarray(lpsUser)\n",
    "\n",
    "def relLP(arr):\n",
    "    return arr/np.mean(arr, axis=1).reshape(np.shape(arr)[0],1)\n",
    "\n",
    "# Get LP\n",
    "lpFTTI_N = getLPNormal(succFTTI, usersFTI)\n",
    "lpFTTU_N = getLPNormal(succFTTU, usersFTU)\n",
    "\n",
    "lpRelFTTI_N = relLP(lpFTTI_N)\n",
    "lpRelFTTU_N = relLP(lpFTTU_N)\n",
    "\n",
    "#user(0), cat-task complexity(1), # task selec(2), % sele(3), # correct on task(4), % correct(5), \n",
    "#answers(6:12), relative performance, lp, relative lp\n",
    "informedFTT = np.column_stack((informedFTT, lpFTTI_N.flatten(), lpRelFTTI_N.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check first selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFirstOption(arr, usersArr, checkIncomplete=False):\n",
    "    firstSelec = []\n",
    "    incomplete = []\n",
    "    for u in range(usersArr):\n",
    "        # If it's necessary to check those that didn't explore all tasks (e.g. free exploration only)\n",
    "        if checkIncomplete:\n",
    "            questions = arr[u]\n",
    "            # Count number of times a task was selected\n",
    "            task, ctask = np.unique(questions[:,1], return_counts=True)\n",
    "\n",
    "            # Check if it explored all tasks\n",
    "            if len(task) < 4:\n",
    "                incomplete.append(u)\n",
    "            else:\n",
    "                firstSelec.append(arr[u][0,1])\n",
    "        else:\n",
    "            firstSelec.append(arr[u][0,1])\n",
    "            \n",
    "    # Return also a list of people who didn't explore all tasks\n",
    "    return np.asarray(firstSelec), incomplete\n",
    "\n",
    "fsFTI,_ = checkFirstOption(splitCsvFTI, usersFTI)\n",
    "fsFTU,_ = checkFirstOption(splitCsvFTU, usersFTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label is whether or not this task was the first option\n",
    "def addLabel(arr, firstSelec):\n",
    "    fsList = []\n",
    "    for i, ii in enumerate(np.arange(0,np.shape(arr)[0],4)):\n",
    "        sliced = arr[ii:ii+4]\n",
    "        fs = sliced[:,1] == firstSelec[i]\n",
    "        fs = fs.astype(int)\n",
    "        fsList.append(fs.tolist())\n",
    "    return np.asarray(fsList)\n",
    "\n",
    "labelFTTI = addLabel(informedFTT, fsFTI)\n",
    "informedFTT = np.column_stack((informedFTT, labelFTTI.flatten()))\n",
    "\n",
    "#user(0), cat-task complexity(1), # task selec(2), % sele(3), # correct on task(4), % correct(5), \n",
    "#answers(6:12), relative performance, lp, relative lp\n",
    "informedFTT[0:4]\n",
    "# Randomize\n",
    "np.take(informedFTT, np.random.permutation(informedFTT.shape[0]), axis=0, out=informedFTT)\n",
    "# Remove column of user id\n",
    "informedFTT = np.delete(informedFTT, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A1'></a>\n",
    "## Decision tree criterions for relavite importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-799b87cd2bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named model_selection"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "# K-fold CV\n",
    "# FOLDS = 3\n",
    "# kf = KFold(n_splits=FOLDS) # Define the split - into 2 folds \n",
    "# kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "# KFold(n_splits=2, random_state=None, shuffle=True)\n",
    "\n",
    "X, Y = informedFTT[:,:-1], informedFTT[:,-1] \n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)\n",
    "\n",
    "\n",
    "# Decision tree\n",
    "classifier = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy is \", accuracy_score(y_test,y_pred)*100)\n",
    "\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier(criterion = \"entropy\")\n",
    "model.fit(X_train, y_train)\n",
    "# Relative importance of each attribute\n",
    "features = ['Complexity', 'Task selection', '% Task selection', '# Correct', '% Correct', 'Reported Interest',\n",
    "           'Reported Complexity', 'Reported Time Invested', 'Reported Progress', 'Reported Rule',\n",
    "            'Reported Future Learning after training', 'Reported Future Learning after experiment',\n",
    "           'Relative Performance', 'LP', 'Relative LP']\n",
    "\n",
    "print(\"==========\")\n",
    "zipped = zip(features, model.feature_importances_)\n",
    "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "for f,i in zipped:\n",
    "    print(f,\" \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A2'></a>\n",
    "## PCA and regressor pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-db2da4c8f6b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named model_selection"
     ]
    }
   ],
   "source": [
    "# PCA + Logistic regression or Decision tree\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X_train_scaled = scale(X_train)\n",
    "\n",
    "#classifier = linear_model.LogisticRegression()\n",
    "classifier = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', classifier)])\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "#X_proj = pca.transform(X_train_scaled)\n",
    "\n",
    "#The amount of variance that each component explains\n",
    "var = pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative explained variance\n",
    "varcum = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "plt.plot(varcum)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('# of components')\n",
    "plt.ylabel('Cumulative Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A3'></a>\n",
    "## Component Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c5b22245cdbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnComp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca__n_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnComp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "nComp = [5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "estimator = GridSearchCV(pipe,dict(pca__n_components=nComp))\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,linestyle='-', label='Components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
